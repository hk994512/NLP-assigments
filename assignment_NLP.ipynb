{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnAByK2opKLl",
        "outputId": "ecaf9d80-ee81-4130-b755-d8f3435e2bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "✅ All packages installed successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install all required packages\n",
        "!pip install nltk beautifulsoup4 requests contractions\n",
        "\n",
        "# Step 2: Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6lJnRIiqPfY",
        "outputId": "3651d2fe-2659-4991-d59e-3371cbea9722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcUoF7DpqbnC",
        "outputId": "e841943e-ddcc-4e9f-88a4-fce0d3a91412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk beautifulsoup4 requests contractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGL31ef0qgjJ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78pR_an5q6WU",
        "outputId": "1fe33652-feee-44fa-fbe8-c1a895d1c096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk beautifulsoup4 requests contractions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4iKwTRirGqA",
        "outputId": "2f021afe-77d0-476b-968b-5aeeffe521e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK9hkhItrTcj",
        "outputId": "4edac44f-a186-408c-b8b9-db3d5821c79c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iih67ArmrvQ4",
        "outputId": "275ca16f-35d9-4c58-d5db-9df877987154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: I can't believe it's working! You're doing great.\n",
            "Lower: i can't believe it's working! you're doing great.\n",
            "Expanded: i cannot believe it is working! you are doing great.\n",
            "No punctuation: i cannot believe it is working you are doing great\n",
            "Tokens: ['i', 'can', 'not', 'believe', 'it', 'is', 'working', 'you', 'are', 'doing', 'great']\n",
            "Without stopwords: ['believe', 'working', 'great']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['believe', 'working', 'great']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions\n",
        "\n",
        "def preprocess(text):\n",
        "    print(\"Original:\", text)\n",
        "\n",
        "    text = text.lower()\n",
        "    print(\"Lower:\", text)\n",
        "\n",
        "    text = contractions.fix(text)\n",
        "    print(\"Expanded:\", text)\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    print(\"No punctuation:\", text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "\n",
        "    sw = set(stopwords.words(\"english\"))\n",
        "    tokens = [t for t in tokens if t not in sw]\n",
        "    print(\"Without stopwords:\", tokens)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "preprocess(\"I can't believe it's working! You're doing great.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TChuWhJsHt5",
        "outputId": "1efc5637-7936-4559-c942-2b8a06ee3419"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Required for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATJ7Yi05scy9",
        "outputId": "446e9073-d780-45a3-d2bb-ba5cb2fadf34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 3: Stemming vs Lemmatization\n",
            "============================================================\n",
            "Comparing stemming vs lemmatization...\n",
            "Word         Porter Stemmer       WordNet Lemmatizer  \n",
            "------------------------------------------------------------\n",
            "running      run                  run                 \n",
            "better       better               better              \n",
            "flies        fli                  fly                 \n",
            "studies      studi                study               \n",
            "amazing      amaz                 amaze               \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Downloads for lemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 3: Stemming vs Lemmatization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def compare_stemming_lemmatization(words):\n",
        "    porter = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    print(f\"{'Word':<12} {'Porter Stemmer':<20} {'WordNet Lemmatizer':<20}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for word in words:\n",
        "        stemmed = porter.stem(word)\n",
        "\n",
        "        # Smart POS-based lemmatization\n",
        "        if word.endswith('ing'):\n",
        "            lemmatized = lemmatizer.lemmatize(word, 'v')  # verb\n",
        "        elif word.endswith('ly'):\n",
        "            lemmatized = lemmatizer.lemmatize(word, 'r')  # adverb\n",
        "        elif word.endswith('s'):\n",
        "            lemmatized = lemmatizer.lemmatize(word, 'n')  # noun\n",
        "        else:\n",
        "            lemmatized = lemmatizer.lemmatize(word)\n",
        "\n",
        "        print(f\"{word:<12} {stemmed:<20} {lemmatized:<20}\")\n",
        "\n",
        "# Test words\n",
        "words_list = [\"running\", \"better\", \"flies\", \"studies\", \"amazing\"]\n",
        "\n",
        "print(\"Comparing stemming vs lemmatization...\")\n",
        "compare_stemming_lemmatization(words_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmFL7rC-siSV",
        "outputId": "5e182a2f-3e4d-44d1-8013-9ee29686f5e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 4: Sentence and Word Tokenization\n",
            "============================================================\n",
            "Tokenizing paragraph into sentences and words...\n",
            " Input paragraph: NLP is amazing. It helps computers understand text. Tokenization is the first step!\n",
            "\n",
            " Number of sentences: 3\n",
            "\n",
            " Structured Output:\n",
            "--------------------------------------------------\n",
            "Sentence 1:\n",
            "   Original: NLP is amazing.\n",
            "   Words: ['NLP', 'is', 'amazing', '.']\n",
            "   Word count: 4\n",
            "\n",
            "Sentence 2:\n",
            "   Original: It helps computers understand text.\n",
            "   Words: ['It', 'helps', 'computers', 'understand', 'text', '.']\n",
            "   Word count: 6\n",
            "\n",
            "Sentence 3:\n",
            "   Original: Tokenization is the first step!\n",
            "   Words: ['Tokenization', 'is', 'the', 'first', 'step', '!']\n",
            "   Word count: 6\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary NLTK functions\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK models (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 4: Sentence and Word Tokenization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def tokenize_paragraph(paragraph):\n",
        "    print(f\" Input paragraph: {paragraph}\")\n",
        "\n",
        "    # Sentence tokenization\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    print(f\"\\n Number of sentences: {len(sentences)}\")\n",
        "\n",
        "    print(\"\\n Structured Output:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        # Word tokenization\n",
        "        words = word_tokenize(sentence)\n",
        "        print(f\"Sentence {i}:\")\n",
        "        print(f\"   Original: {sentence}\")\n",
        "        print(f\"   Words: {words}\")\n",
        "        print(f\"   Word count: {len(words)}\")\n",
        "        print()\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"NLP is amazing. It helps computers understand text. Tokenization is the first step!\"\n",
        "print(\"Tokenizing paragraph into sentences and words...\")\n",
        "tokenize_paragraph(paragraph)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCg-L7zstGhm",
        "outputId": "09bcdb9f-329a-41ac-d6a5-8d8c5b4d2173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 5: Custom Stopword Removal\n",
            "============================================================\n",
            "Removing custom stopwords...\n",
            " Original text: NLP processing requires data processing and understanding of language.\n",
            " Tokens: ['nlp', 'processing', 'requires', 'data', 'processing', 'and', 'understanding', 'of', 'language', '.']\n",
            " Custom stopwords: ['nlp', 'data', 'processing']\n",
            " Filtered tokens: ['requires', 'understanding', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 5: Custom Stopword Removal\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def remove_custom_stopwords(text, custom_stopwords):\n",
        "    print(f\" Original text: {text}\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    print(f\" Tokens: {tokens}\")\n",
        "\n",
        "    # Combine default and custom stopwords\n",
        "    default_stopwords = set(stopwords.words('english'))\n",
        "    all_stopwords = default_stopwords.union(set(custom_stopwords))\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in all_stopwords]\n",
        "\n",
        "    print(f\" Custom stopwords: {custom_stopwords}\")\n",
        "    print(f\" Filtered tokens: {filtered_tokens}\")\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Sample text and custom stopwords\n",
        "sample_text_5 = \"NLP processing requires data processing and understanding of language.\"\n",
        "custom_stopwords = ['nlp', 'data', 'processing']\n",
        "\n",
        "print(\"Removing custom stopwords...\")\n",
        "filtered_result = remove_custom_stopwords(sample_text_5, custom_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrL7EhPPtSSS",
        "outputId": "624d6f56-40bf-46b2-dbeb-3039bb48d725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 6: Stemmer Comparison\n",
            "============================================================\n",
            "Comparing different stemmers...\n",
            "Word            Porter       Lancaster    Regex       \n",
            "-------------------------------------------------------\n",
            "running         run          run          runn        \n",
            "happily         happili      happy        happi       \n",
            "studies         studi        study        studie      \n",
            "computational   comput       comput       computational\n",
            "flying          fli          fly          fly         \n",
            "\n",
            "========================================\n",
            " JUSTIFICATION:\n",
            "========================================\n",
            "\n",
            "Porter Stemmer: \n",
            "   Most commonly used, moderate aggression\n",
            "   Good balance between stemming and readability\n",
            "   Best for general purpose applications\n",
            "\n",
            "Lancaster Stemmer:\n",
            "   More aggressive, often over-stems\n",
            "   Faster but may reduce words to unrecognizable forms\n",
            "   Good for search applications where recall is important\n",
            "\n",
            "Regex Stemmer:\n",
            "   Customizable but limited to predefined patterns\n",
            "   May miss complex morphological changes\n",
            "   Good for specific domains with known patterns\n",
            "\n",
            " Conclusion: Porter Stemmer is generally considered best for most NLP tasks\n",
            "due to its balance between aggression and readability.\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary NLTK stemmers\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer\n",
        "import nltk\n",
        "\n",
        "# Download punkt if needed (sometimes required for tokenization before stemming)\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 6: Stemmer Comparison\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def compare_stemmers(words):\n",
        "    porter = PorterStemmer()\n",
        "    lancaster = LancasterStemmer()\n",
        "    regex_stemmer = RegexpStemmer('ing$|s$|e$|able$|ly$', min=4)\n",
        "\n",
        "    print(f\"{'Word':<15} {'Porter':<12} {'Lancaster':<12} {'Regex':<12}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    for word in words:\n",
        "        porter_stem = porter.stem(word)\n",
        "        lancaster_stem = lancaster.stem(word)\n",
        "        regex_stem = regex_stemmer.stem(word)\n",
        "\n",
        "        print(f\"{word:<15} {porter_stem:<12} {lancaster_stem:<12} {regex_stem:<12}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\" JUSTIFICATION:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"\"\"\n",
        "Porter Stemmer:\n",
        "   Most commonly used, moderate aggression\n",
        "   Good balance between stemming and readability\n",
        "   Best for general purpose applications\n",
        "\n",
        "Lancaster Stemmer:\n",
        "   More aggressive, often over-stems\n",
        "   Faster but may reduce words to unrecognizable forms\n",
        "   Good for search applications where recall is important\n",
        "\n",
        "Regex Stemmer:\n",
        "   Customizable but limited to predefined patterns\n",
        "   May miss complex morphological changes\n",
        "   Good for specific domains with known patterns\n",
        "\n",
        " Conclusion: Porter Stemmer is generally considered best for most NLP tasks\n",
        "due to its balance between aggression and readability.\n",
        "    \"\"\")\n",
        "\n",
        "# Sample words for stemming\n",
        "words_to_stem = ['running', 'happily', 'studies', 'computational', 'flying']\n",
        "print(\"Comparing different stemmers...\")\n",
        "compare_stemmers(words_to_stem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQSvfyQLtqzO",
        "outputId": "4832398a-6b3b-42dc-cf2f-1c9ece782d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 7: HTML Text Extraction\n",
            "============================================================\n",
            " Extracting text from HTML...\n",
            "Extracted text from HTML:\n",
            "--------------------------------------------------\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. It focuses on how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents. NLP technologies have become increasingly important in today's world, powering applications like virtual assistants, translation services, and ...\n",
            "\n",
            "Total characters extracted: 525\n"
          ]
        }
      ],
      "source": [
        "# Import BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 7: HTML Text Extraction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def extract_html_text():\n",
        "    # Using sample HTML content\n",
        "    sample_html = \"\"\"\n",
        "    <html>\n",
        "    <head><title>NLP Information</title></head>\n",
        "    <body>\n",
        "    <h1>Natural Language Processing</h1>\n",
        "    <p>Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.</p>\n",
        "    <p>It focuses on how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of understanding the contents of documents.</p>\n",
        "    <p>NLP technologies have become increasingly important in today's world, powering applications like virtual assistants, translation services, and sentiment analysis tools.</p>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse HTML with BeautifulSoup\n",
        "    soup = BeautifulSoup(sample_html, 'html.parser')\n",
        "\n",
        "    # Extract all paragraph text\n",
        "    paragraphs = soup.find_all('p')\n",
        "    extracted_text = \" \".join([p.get_text().strip() for p in paragraphs])\n",
        "\n",
        "    print(\"Extracted text from HTML:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(extracted_text[:500] + \"...\" if len(extracted_text) > 500 else extracted_text)\n",
        "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "print(\" Extracting text from HTML...\")\n",
        "html_text = extract_html_text()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yez3eE7yuMXo",
        "outputId": "4c068e0d-2aab-45bd-954a-b651c0aeba20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Q1. Part 8: Text Normalization and Lemmatization\n",
            "============================================================\n",
            "Normalizing and lemmatizing text...\n",
            " Original text: Éducation is essential for improving lives. The students are studying harder.\n",
            "1. Lowercase: éducation is essential for improving lives. the students are studying harder.\n",
            "2. No accents: education is essential for improving lives. the students are studying harder.\n",
            "3. Lemmatized tokens: ['education', 'is', 'essential', 'for', 'improve', 'life', '.', 'the', 'student', 'are', 'study', 'harder', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Q1. Part 8: Text Normalization and Lemmatization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def normalize_and_lemmatize(text):\n",
        "    print(f\" Original text: {text}\")\n",
        "\n",
        "    # 1. Convert to lowercase\n",
        "    text_lower = text.lower()\n",
        "    print(f\"1. Lowercase: {text_lower}\")\n",
        "\n",
        "    # 2. Remove accented characters\n",
        "    text_normalized = unicodedata.normalize('NFKD', text_lower)\n",
        "    text_no_accent = ''.join([c for c in text_normalized if not unicodedata.combining(c)])\n",
        "    print(f\"2. No accents: {text_no_accent}\")\n",
        "\n",
        "    # 3. Tokenize\n",
        "    tokens = word_tokenize(text_no_accent)\n",
        "\n",
        "    # 4. Lemmatize with simple POS tagging\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.endswith('ed') or token.endswith('ing'):\n",
        "            lemma = lemmatizer.lemmatize(token, 'v')  # verb\n",
        "        elif token.endswith('s'):\n",
        "            lemma = lemmatizer.lemmatize(token, 'n')  # noun\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token)  # default to noun\n",
        "\n",
        "        lemmatized_tokens.append(lemma)\n",
        "\n",
        "    print(f\"3. Lemmatized tokens: {lemmatized_tokens}\")\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Sample text with accented characters\n",
        "sample_text_8 = \"Éducation is essential for improving lives. The students are studying harder.\"\n",
        "print(\"Normalizing and lemmatizing text...\")\n",
        "final_tokens_8 = normalize_and_lemmatize(sample_text_8)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
